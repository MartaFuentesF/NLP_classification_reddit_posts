{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a50b0a-e129-46de-bbe9-a827271fc3b9",
   "metadata": {},
   "source": [
    "# Project 3: API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47aeac-da1a-4feb-bfb2-b3ab805f3174",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "For Project 3, the objective is to collect and analyze posts from two distinct subreddits. Subreddits are specialized forums on the popular website Reddit that cater to specific topics and interests.\n",
    "\n",
    "The focus will be on creating a classification model to discern the origin of a subreddit post between the following two subreddits:\n",
    "\n",
    "**Subreddit 1: [r/AmItheAsshole]('https://www.reddit.com/r/AmItheAsshole/')**\n",
    "\n",
    "Description: Serving as a platform for moral introspection, r/AmItheAsshole provides a space for individuals to seek feedback on contentious issues. Users present both sides of a dilemma and solicit opinions to ascertain whether their actions align with societal norms.\n",
    "\n",
    "\n",
    "**Subreddit 2: [r/AskLawyers]('https://www.reddit.com/r/AskLawyers/')**\n",
    "\n",
    "Description: Offering legal guidance, r/AskLawyers encourages users to pose questions with the understanding that professional legal advice requires consultation with an attorney. Public posts and comments in this subreddit are not construed as forming an attorney-client relationship.\n",
    "\n",
    "\n",
    "The selection of these two subreddits stems from their shared purpose of providing a venue for individuals to seek input on personal matters. While one focuses on ethical considerations and societal expectations, the other centers on legal principles and potential courses of action. This juxtaposition between moral inquiry and legal inquiry presents an intriguing avenue for exploration, highlighting the intersection of public opinion and formal legal frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f2c533-0b1b-4c11-a1f5-246621f87693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# this setting widens how many characters pandas will display in a column:\n",
    "pd.options.display.max_colwidth = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed973e-23c8-40e2-84d0-54a205bcf67b",
   "metadata": {},
   "source": [
    "## Authorizing a Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18deb044-6532-4832-920c-5e88a6a8d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set credentials\n",
    "client_id = 'g9LyrhpQZmvO3B_erOWQyQ' #alphanumeric string provided under \"personal use script\"\n",
    "client_secret = '9hr0j6-ofILVfxsCPW3CuJbCKyAS3Q'  #alphanumeric string provided as \"secret\"\n",
    "user_agent = 'Project 3' #the name of your application\n",
    "username = 'Soft_Ad_116' #your reddit username\n",
    "password = 'lascondes' #your reddit password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3362ff5b-702a-44c9-8489-254091b2f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requesting authorization \n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': password\n",
    "}\n",
    "   \n",
    "#create an informative header for my application\n",
    "headers = {'User-Agent': 'namehere/0.0.1'}\n",
    "\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth,\n",
    "    data=data,\n",
    "    headers=headers)\n",
    "\n",
    "#retrieve access token\n",
    "token = res.json()['access_token']\n",
    "\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e18198-e911-4281-9a6a-a9e44d1362bb",
   "metadata": {},
   "source": [
    "## Collecting Data from Subreddit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e52a8-bbc9-432e-bbec-d016cc7d5b72",
   "metadata": {},
   "source": [
    "In the code below, I state the subreddit I want to use in the `subreddit` variable.\n",
    "Then, I request 100 posts 10 times and use a dictionary method to create a data frame for each scrape. Each data frame is saved to a CSV. \n",
    "\n",
    "Note: To run correctly, the requests need the parameter `after`, which takes an ID code from the previous scrape. A boolean variable named `label` ensures the code runs the first time. \n",
    "\n",
    "Citations:\n",
    "* Rowan Schaefer helped me make the request loop.\n",
    "* [datetime function]('https://www.toppr.com/guides/python-guide/tutorials/python-date-and-time/datetime/current-datetime/how-to-get-current-date-and-time-in-python/#:~:text=The%20datetime%20module's%20now(),dd%20hh%3Amm%3Ass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada7e5e6-8bd7-40aa-bfd5-5c49d676b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the site we are connecting to\n",
    "base_url = 'https://oauth.reddit.com/r/'\n",
    "subreddit = 'AmItheAsshole'\n",
    "\n",
    "# Getting the posts\n",
    "label = False  # ensures the loop runs the first time\n",
    "for request in range(10): \n",
    "    \n",
    "    time.sleep(2) # wait 2 seconds between runs\n",
    "    if label:\n",
    "        # set parameters\n",
    "        params = {\n",
    "            'limit': 100,\n",
    "            'after': after_label\n",
    "        }\n",
    "        \n",
    "        # make the request\n",
    "        res = requests.get(base_url + subreddit, \n",
    "                   headers = headers,\n",
    "                   params = params)\n",
    "   \n",
    "    else:      # only runs in the first iteration of the loop\n",
    "        # parameters with no 'after'\n",
    "        params = {'limit': 100}\n",
    "\n",
    "        # makes first request, w/o the 'after' parameter\n",
    "        res = requests.get(base_url + subreddit, \n",
    "                   headers = headers,\n",
    "                   params = params)\n",
    "        \n",
    "    \n",
    "    # sets the 'after' parameter\n",
    "    after_label = res.json()['data']['after'] \n",
    "    label = True\n",
    "\n",
    "    \n",
    "    # Making a Data Frame using the information from the 100 posts\n",
    "    \n",
    "    posts = [] # list of dictionaries to store post data\n",
    "\n",
    "    #looping through posts to get pertinent data\n",
    "    no_of_posts = len(res.json()['data']['children']) \n",
    "    \n",
    "    for i in range(no_of_posts):\n",
    "        post_title = res.json()['data']['children'][i]['data']['title']\n",
    "        post_text = res.json()['data']['children'][i]['data']['selftext']\n",
    "        post_source = res.json()['data']['children'][i]['data']['subreddit']\n",
    "\n",
    "        posts.append({'title': post_title, 'post': post_text, 'source' : post_source})\n",
    "\n",
    "    #creating a Pandas DataFrame using 'posts'\n",
    "    df = pd.DataFrame(posts) \n",
    "    \n",
    "    # Storing the current date in a variable to use in csv file name\n",
    "    date = datetime.now().strftime('%m-%d')\n",
    "\n",
    "    # saving data frame to csv\n",
    "    df.to_csv(f'../project-3/Data/df{date}{str(request)}.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4361e7-af53-4c5e-8f94-95eee1a2efe1",
   "metadata": {},
   "source": [
    "## Collecting Data from Subreddit 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0f9cf-ae7f-4489-80da-7a5ad74b449b",
   "metadata": {},
   "source": [
    "In the code below, I state the subreddit I want to use in the `subreddit` variable.\n",
    "Then, I request 100 posts 10 times and use a dictionary method to create a data frame for each scrape. Each data frame is saved to a CSV. \n",
    "\n",
    "Note: To run correctly, the requests need the parameter `after`, which takes an ID code from the previous scrape. A boolean variable named `label` ensures the code runs the first time. \n",
    "\n",
    "Citations:\n",
    "* Rowan Schaefer helped me make the request loop.\n",
    "* [datetime function]('https://www.toppr.com/guides/python-guide/tutorials/python-date-and-time/datetime/current-datetime/how-to-get-current-date-and-time-in-python/#:~:text=The%20datetime%20module's%20now(),dd%20hh%3Amm%3Ass.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28efb336-ed42-44af-b70f-83e73a4ce2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the site we are connecting to\n",
    "base_url = 'https://oauth.reddit.com/r/'\n",
    "subreddit = 'AskLawyers'\n",
    "\n",
    "\n",
    "label = False  # ensures the loop runs the first time\n",
    "\n",
    "for request in range(10): \n",
    "    \n",
    "    time.sleep(2) # wait 2 seconds between runs\n",
    "    if label:\n",
    "        # set parameters\n",
    "        params = {\n",
    "            'limit': 100,\n",
    "            'after': after_label\n",
    "        }\n",
    "        \n",
    "        # make the request\n",
    "        res = requests.get(base_url + subreddit, \n",
    "                   headers = headers,\n",
    "                   params = params)\n",
    "   \n",
    "    else:      # only runs in the first iteration of the loop\n",
    "        # parameters with no 'after'\n",
    "        params = {'limit': 100}\n",
    "\n",
    "        # makes first request, w/o the 'after' parameter\n",
    "        res = requests.get(base_url+subreddit, \n",
    "                   headers=headers,\n",
    "                   params= params)\n",
    "        \n",
    "    \n",
    "    # sets the 'after' parameter\n",
    "    after_label = res.json()['data']['after'] \n",
    "    label = True\n",
    "\n",
    "    \n",
    "    # Making a Data Frame using the information from the 100 posts\n",
    "    \n",
    "    posts = [] # list of dictionaries to store post data\n",
    "\n",
    "    #looping through posts to get pertinent data\n",
    "    no_of_posts = len(res.json()['data']['children']) \n",
    "    \n",
    "    for i in range(no_of_posts):\n",
    "        post_title = res.json()['data']['children'][i]['data']['title']\n",
    "        post_text = res.json()['data']['children'][i]['data']['selftext']\n",
    "        post_source = res.json()['data']['children'][i]['data']['subreddit']\n",
    "\n",
    "        posts.append({'title': post_title, 'post': post_text, 'source' : post_source})\n",
    "\n",
    "    #creating a Pandas DataFrame using 'posts'\n",
    "    df = pd.DataFrame(posts) \n",
    "    \n",
    "    # Storing the current date in a variable to use in csv file name\n",
    "    date = datetime.now().strftime('%m-%d')\n",
    "\n",
    "    # saving data frame to csv\n",
    "    df.to_csv(f'../project-3/Data/df{date}{str(request)}.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc5db7-afc1-41f8-8fcf-0e5869d672d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
